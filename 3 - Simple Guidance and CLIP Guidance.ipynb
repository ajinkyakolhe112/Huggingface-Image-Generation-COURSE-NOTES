{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets, diffusers, torchvision\n",
    "\n",
    "from datasets import load_dataset\n",
    "from diffusers import DDIMScheduler, DDPMPipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "device = (\"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:24<00:00,  6.09s/it]\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  6.78it/s]\n"
     ]
    }
   ],
   "source": [
    "image_generation_pipeline = diffusers.DDPMPipeline.from_pretrained(pretrained_model_name_or_path = \"johnowhitaker/sd-class-wikiart-from-bedrooms\", )\n",
    "image_generation_pipeline.to(device);\n",
    "\n",
    "# Create new scheduler and set num inference steps\n",
    "scheduler = diffusers.DDIMScheduler.from_pretrained(\"johnowhitaker/sd-class-wikiart-from-bedrooms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_loss(images, target_color=(0.1, 0.9, 0.5)):\n",
    "    \"\"\"Given a target color (R, G, B) return a loss for how far away on average\n",
    "    the images' pixels are from that color. Defaults to a light teal: (0.1, 0.9, 0.5)\"\"\"\n",
    "\n",
    "    target = torch.tensor(target_color).to(images.device) * 2 - 1   # Map target color to (-1, 1)\n",
    "    target = target[None, :, None, None]                            # Get shape right to work with the images (b, c, h, w)\n",
    "    error  = torch.abs(images - target).mean()                      # Mean absolute difference between the image pixels and the target color\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The guidance scale determines the strength of the effect\n",
    "guidance_loss_scale = 40  # Explore changing this to 5, or 100\n",
    "\n",
    "x = torch.randn(8, 3, 256, 256).to(device)\n",
    "\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "\n",
    "    # Prepare the model input\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_generation_pipeline.unet(model_input, t)[\"sample\"]\n",
    "\n",
    "    # Set x.requires_grad to True\n",
    "    x = x.detach().requires_grad_()\n",
    "\n",
    "    # Get the predicted x0\n",
    "    x0 = scheduler.step(noise_pred, t, x).pred_original_sample\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = color_loss(x0) * guidance_loss_scale\n",
    "    if i % 10 == 0:\n",
    "        print(i, \"loss:\", loss.item())\n",
    "\n",
    "    # Get gradient\n",
    "    cond_grad = -torch.autograd.grad(loss, x)[0]\n",
    "\n",
    "    # Modify x based on this gradient\n",
    "    x = x.detach() + cond_grad\n",
    "\n",
    "    # Now step with scheduler\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "# View the output\n",
    "grid = torchvision.utils.make_grid(x, nrow=4)\n",
    "im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5\n",
    "Image.fromarray(np.array(im * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Guidance, CLIP(Text & Image) Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "image_generation_pipeline = diffusers.DDPMPipeline.from_pretrained(pretrained_model_name_or_path = \"johnowhitaker/sd-class-wikiart-from-bedrooms\", )\n",
    "image_generation_pipeline.to(device);\n",
    "\n",
    "# Create new scheduler and set num inference steps\n",
    "scheduler = diffusers.DDIMScheduler.from_pretrained(\"johnowhitaker/sd-class-wikiart-from-bedrooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "clip        = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor   = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "text_descriptions  = [' Red Rose (still life), red flower painting ']\n",
    "url         = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "images       = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs  = processor(text= text_descriptions, images= images, return_tensors=\"pt\", padding=True)\n",
    "outputs = clip(**inputs)\n",
    "\n",
    "outputs.text_embeds, outputs.image_embeds\n",
    "\n",
    "def clip_loss(image_embeds, text_embeds):\n",
    "    dists = image_embeds.sub(text_embeds).norm(dim=2).div(2).arcsin().pow(2).mul(2)  # Squared Great Circle Distance\n",
    "    return dists.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pay attention to `repeat_guidance_counter`**\n",
    "\n",
    "- `repeat_guidance_counter` provides a knob to fine-tune the trade-off between stability and the strength of CLIP's influence on the generated image. \n",
    "- Higher values of `repeat_guidance_counter` can lead to smoother gradients and potentially better image quality, but at the cost of increased computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUIDANCE_PROMPT = \"Red Rose (still life), red flower painting\"  # @param\n",
    "\n",
    "# Explore changing this\n",
    "guidance_scale          = 8  # @param\n",
    "repeat_guidance_counter = 4  # @param\n",
    "\n",
    "# More steps -> more time for the guidance to have an effect\n",
    "scheduler.set_timesteps(50)\n",
    "\n",
    "x = torch.randn(4, 3, 256, 256).to(device)  # RAM usage is high, you may want only 1 image at a time\n",
    "\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_generation_pipeline.unet(model_input, t)[\"sample\"]\n",
    "\n",
    "    cond_grad = 0\n",
    "\n",
    "    for iteration_number in range(repeat_guidance_counter):\n",
    "\n",
    "        # Set requires grad on x\n",
    "        x = x.detach().requires_grad_()\n",
    "\n",
    "        # Get the predicted x0:\n",
    "        x0 = scheduler.step(noise_pred, t, x).pred_original_sample\n",
    "\n",
    "        # EXTERNAL CLIP MODEL BEING USED HERE\n",
    "        inputs      = processor( text = GUIDANCE_PROMPT , images = x0 , return_tensors = 'pt', padding=True )\n",
    "        outputs     = clip(**inputs)\n",
    "        # USE THESE -> outputs.image_embeds & outputs.text_embeds\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = clip_loss(outputs.image_embeds, outputs.text_embeds) * guidance_scale\n",
    "\n",
    "        # Get gradient (scale by n_cuts since we want the average)\n",
    "        cond_grad -= torch.autograd.grad(loss, x)[0] / iteration_number\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(\"Step:\", i, \", Guidance loss:\", loss.item())\n",
    "\n",
    "    # Modify x based on this gradient\n",
    "    alpha_bar = scheduler.alphas_cumprod[i]\n",
    "    x = x.detach() + cond_grad * alpha_bar.sqrt()  # Note the additional scaling factor here!\n",
    "\n",
    "    # Now step with scheduler\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "\n",
    "grid = torchvision.utils.make_grid(x.detach(), nrow=4)\n",
    "im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5\n",
    "Image.fromarray(np.array(im * 255).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
