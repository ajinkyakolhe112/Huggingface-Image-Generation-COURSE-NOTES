{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets, diffusers, torchvision\n",
    "\n",
    "from datasets import load_dataset\n",
    "from diffusers import DDIMScheduler, DDPMPipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "device = (\"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:05<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "image_generation_pipeline = diffusers.DDPMPipeline.from_pretrained(pretrained_model_name_or_path = \"google/ddpm-celebahq-256\", )\n",
    "image_generation_pipeline.to(device);\n",
    "\n",
    "# Create new scheduler and set num inference steps\n",
    "scheduler = diffusers.DDIMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning - Start: Celebrity Faces, End: Anime Faces\n",
    "\n",
    "#### What is Fine Tuning & Why Fine Tuning?\n",
    "1. Get much higher accuracy for a specific use case\n",
    "2. Fine tuning adds extra knowledge on existing dataset. Hence start dataset should have foundational knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_internal_dict', 'unet', 'scheduler'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(image_generation_pipeline).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generation_pipeline = diffusers.DDPMPipeline.from_pretrained(pretrained_model_name_or_path = \"google/ddpm-celebahq-256\", )\n",
    "scheduler                 = diffusers.DDIMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "\n",
    "dataset_name = \"huggan/anime-faces\"  \n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "image_size = 256  # @param\n",
    "batch_size = 4  # @param\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Previewing batch:\")\n",
    "batch = next(iter(train_dataloader))\n",
    "grid = torchvision.utils.make_grid(batch[\"images\"], nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2  # @param\n",
    "lr = 1e-5  # 2param\n",
    "grad_accumulation_steps = 2  # @param\n",
    "\n",
    "optimizer = torch.optim.AdamW(image_generation_pipeline.unet.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            image_generation_pipeline.scheduler.num_train_timesteps,\n",
    "            (bs,),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = image_generation_pipeline.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction for the noise\n",
    "        noise_pred = image_generation_pipeline.unet(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # Compare the prediction with the actual noise:\n",
    "        loss = F.mse_loss(\n",
    "            noise_pred, noise\n",
    "        )  # NB - trying to predict noise (eps) not (noisy_ims-clean_ims) or just (clean_ims)\n",
    "\n",
    "        # Store for later plotting\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update the model parameters with the optimizer based on this loss\n",
    "        loss.backward(loss)\n",
    "\n",
    "        # Gradient accumulation:\n",
    "        if (step + 1) % grad_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch} average loss: {sum(losses[-len(train_dataloader):])/len(train_dataloader)}\")\n",
    "\n",
    "# Plot the loss curve:\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'timesteps => {scheduler.timesteps}')\n",
    "\n",
    "x = torch.randn(8, 3, 256, 256).to(device)  # Batch of 8\n",
    "\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_generation_pipeline.unet(model_input, t)[\"sample\"]\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "grid = torchvision.utils.make_grid(x, nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning in one Cell - Start: Bedrooms, Fine-tune: Wikiart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def train(start_model = \"google/ddpm-bedroom-256\", dataset_name = \"huggan/wikiart\",\n",
    "                image_size = 256, batch_size = 16, grad_accumulation_steps = 2, num_epochs = 1, device = 'cuda', model_save_name = 'wikiart_1e', wandb_project = 'dm_finetune', log_samples_every = 250, save_model_every = 2500,):\n",
    "\n",
    "    device = \"cpu\"   \n",
    "    # Initialize wandb for logging\n",
    "    wandb.init(project=wandb_project, config=locals())\n",
    "\n",
    "\n",
    "    # Prepare pretrained model\n",
    "    image_pipe = DDPMPipeline.from_pretrained(start_model);\n",
    "    image_pipe.to(device)\n",
    "    \n",
    "    # Get a scheduler for sampling\n",
    "    sampling_scheduler = DDIMScheduler.from_config(start_model)\n",
    "    sampling_scheduler.set_timesteps(num_inference_steps=50)\n",
    "\n",
    "    # Prepare dataset\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    def transform(examples):\n",
    "        images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "        return {\"images\": images}\n",
    "    dataset.set_transform(transform)\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    # Optimizer & lr scheduler\n",
    "    optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "\n",
    "            # Get the clean images\n",
    "            clean_images = batch['images'].to(device)\n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, image_pipe.scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            # Get the model prediction for the noise\n",
    "            noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "            # Compare the prediction with the actual noise:\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            # Log the loss\n",
    "            wandb.log({'loss':loss.item()})\n",
    "\n",
    "            # Calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Acccumulation: Only update every grad_accumulation_steps \n",
    "            if (step+1)%grad_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            # Occasionally log samples\n",
    "            if (step+1)%log_samples_every == 0:\n",
    "                x = torch.randn(8, 3, 256, 256).to(device) # Batch of 8\n",
    "                for i, t in tqdm(enumerate(sampling_scheduler.timesteps)):\n",
    "                    model_input = sampling_scheduler.scale_model_input(x, t)\n",
    "                    with torch.no_grad():\n",
    "                        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "                    x = sampling_scheduler.step(noise_pred, t, x).prev_sample\n",
    "                grid = torchvision.utils.make_grid(x, nrow=4)\n",
    "                im = grid.permute(1, 2, 0).cpu().clip(-1, 1)*0.5 + 0.5\n",
    "                im = Image.fromarray(np.array(im*255).astype(np.uint8))\n",
    "                wandb.log({'Sample generations': wandb.Image(im)})\n",
    "                \n",
    "            # Occasionally save model\n",
    "            if (step+1)%save_model_every == 0:\n",
    "                image_pipe.save_pretrained(model_save_name+f'step_{step+1}')\n",
    "\n",
    "        # Update the learning rate for the next epoch\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the pipeline one last time\n",
    "    image_pipe.save_pretrained(model_save_name)\n",
    "    \n",
    "    # Wrap up the run\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
